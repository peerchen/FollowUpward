{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cpc-linear.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcae/FollowUpward/blob/master/cpc_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rxh3QO6V-NQI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "gXGR65FP9_5b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fWJPHDB-Wt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jan 20 18:40:09 2019\n",
        "\n",
        "@author: tc\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "# import pandas as pd\n",
        "from targets_features import TfVectors\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#cur_cand = ['xrp_usdt', 'btc_usdt']\n",
        "CUR_PAIR = 'xrp_usdt'\n",
        "VAL_RATIO = 0.1\n",
        "tf_vectors = None\n",
        "\n",
        "def load_targets_features(currency_pair: str):\n",
        "    \"\"\"converts historic catalyst data of currency pairs into classifier target and feature vectors\n",
        "    and stores them for classifier training and evaluation\n",
        "    \"\"\"\n",
        "    fname = os.getcwd() + '/' + currency_pair + '.pydata'\n",
        "    tf_vec = TfVectors(filename=fname)\n",
        "    assert tf_vec is not None, \"missing tf vectors from {fname}\"\n",
        "    return tf_vec\n",
        "\n",
        "\n",
        "\n",
        "def construct_feature_columns(input_features):\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Args:\n",
        "    input_features: The names of the numerical input features to use.\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\"\n",
        "  return set([tf.feature_column.numeric_column(my_feature)\n",
        "              for my_feature in input_features])\n",
        "\n",
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a linear classification model.\n",
        "\n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}\n",
        "\n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "\n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(7000)\n",
        "\n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "\n",
        "def train_linear_classifier_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model.\n",
        "\n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "\n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns\n",
        "      to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column\n",
        "      to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns\n",
        "       to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column\n",
        "       to use as target for validation.\n",
        "\n",
        "  Returns:\n",
        "    A `LinearClassifier` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "\n",
        "  # Create a linear classifier object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      optimizer=my_optimizer, model_dir='/Users/tc/tf_models/crypto'\n",
        "  )\n",
        "  print(f\"model directory: {linear_classifier.model_dir}\")\n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda: my_input_fn(training_examples,\n",
        "                                          training_targets,\n",
        "                                          batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(training_examples,\n",
        "                                                  training_targets,\n",
        "                                                  num_epochs=1,\n",
        "                                                  shuffle=False)\n",
        "  predict_validation_input_fn = lambda: my_input_fn(validation_examples,\n",
        "                                                    validation_targets,\n",
        "                                                    num_epochs=1,\n",
        "                                                    shuffle=False)\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss (on training data):\")\n",
        "  training_log_losses = []\n",
        "  validation_log_losses = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    # Take a break and compute predictions.\n",
        "    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
        "\n",
        "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
        "\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, training_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_log_losses.append(training_log_loss)\n",
        "    validation_log_losses.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_log_losses, label=\"training\")\n",
        "  plt.plot(validation_log_losses, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_classifier\n",
        "\n",
        "\n",
        "tf_vectors = load_targets_features(CUR_PAIR)\n",
        "set_dict = tf_vectors.split_data(1)\n",
        "print(set_dict['training'].describe())\n",
        "print(set_dict['validation'].describe())\n",
        "print(set_dict['test'].describe())\n",
        "c_df = tf_vectors.vec(1) # focus on the T1 aggregation\n",
        "val_count = math.ceil(len(c_df) * VAL_RATIO) # number of validation samples\n",
        "train_count = math.floor(len(c_df) * (1-VAL_RATIO)) # nbr of training samples\n",
        "f_list = list(c_df.columns) # column names of features\n",
        "f_list.remove('close')\n",
        "f_list.remove('buy')\n",
        "f_list.remove('sell')\n",
        "f_df = c_df[f_list] # features dataframe\n",
        "b_df = c_df['buy'] # buy target dataframe\n",
        "s_df = c_df['sell'] # sell target dataframe\n",
        "f_train_df = f_df.head(train_count) # features training dataframe\n",
        "b_tgt_train_df = b_df.head(train_count) # buy target training dataframe\n",
        "s_tgt_train_df = b_df.head(train_count) # sell target training dataframe\n",
        "f_val_df = f_df.tail(val_count) # features validation dataframe\n",
        "b_tgt_val_df = b_df.tail(val_count) # buy validation dataframe\n",
        "s_tgt_val_df = s_df.tail(val_count) # sell validation dataframe\n",
        "\n",
        "\n",
        "b_linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000005,\n",
        "    steps=500,\n",
        "    batch_size=20,\n",
        "    training_examples=f_train_df,\n",
        "    training_targets=b_tgt_train_df,\n",
        "    validation_examples=f_val_df,\n",
        "    validation_targets=b_tgt_val_df)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XO1Khugw-YeY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jan  7 21:43:26 2019\n",
        "\n",
        "@author: tc\n",
        "\"\"\"\n",
        "#import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "BUY = 'buy'\n",
        "HOLD = '-'\n",
        "SELL = 'sell'\n",
        "FEE = 1 #  in per mille, transaction fee is 0.1%\n",
        "BUY_THRESHOLD = 10 # in per mille\n",
        "SELL_THRESHOLD = -2 # in per mille\n",
        "VOL_BASE_PERIOD = '1D'\n",
        "CPC = 'CPC'\n",
        "\n",
        "def time_in_index(dataframe_with_timeseriesindex, tic):\n",
        "    return True in dataframe_with_timeseriesindex.index.isin([tic])\n",
        "\n",
        "class TargetsFeatures:\n",
        "    \"\"\"Receives a dict of currency pairs with associated minute candle data and\n",
        "    transforms it into a dict of currency pairs with associated dicts of\n",
        "    time_aggregations features. The time aggregation is the dict key with one\n",
        "    special key 'CPC' that provides the summary targets\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    time_aggregations:\n",
        "        dict with required time aggregation keys and associated number\n",
        "        of periods that shall be compiled in a corresponding feature vector\n",
        "    minute_data:\n",
        "        currency pair (as keys) dict of input minute data as corresponding\n",
        "        pandas DataFrame\n",
        "\n",
        "    To Do\n",
        "    =====\n",
        "    buy - sell signals:\n",
        "        now reduced to first signal\n",
        "        rolling shall allow a richer buy - sell signalling that are simply mapped on a\n",
        "        common timeline where the optimization problem is addressed\n",
        "\n",
        "    >>> read dataframe from file\n",
        "    >>> concatenate features to full feature vector and write it as file\n",
        "\n",
        "    abbreviatons and terms\n",
        "    ======================\n",
        "    time aggregation - time period for which features are derived, e.g. open, close, high, low.\n",
        "    In this context different time aggregations are used to low pass filter high frequent\n",
        "    volatility.\n",
        "    cpc - currency pair classifier\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, minute_dataframe, aggregation=None, cur_pair=None):\n",
        "        assert not minute_dataframe.empty, \"empty dataframe\"\n",
        "        if aggregation:\n",
        "            self.time_aggregations = aggregation\n",
        "        else:\n",
        "            self.time_aggregations = {CPC: 0, 1: 10, 5: 10, \\\n",
        "                                      15: 10, 60: 10, 4*60: 10}\n",
        "            # keys is aggregation in minutes, value is nbr of trailing DHTBV values\n",
        "        self.performance = self.time_aggregations.copy()\n",
        "        self.minute_data = pd.DataFrame()\n",
        "        self.tf_aggs = dict() # feature and target aggregations\n",
        "        self.minute_data = minute_dataframe\n",
        "\n",
        "\n",
        "        self.time_aggregation() # calculate features and time aggregations of features\n",
        "        self.missed_buy_end = 0 # currently unused\n",
        "        self.missed_sell_start = 0 # currently unused\n",
        "        for time_agg in self.time_aggregations:\n",
        "            if isinstance(time_agg, int):\n",
        "                self.add_period_specific_targets(time_agg) # add aggregation targets\n",
        "        print(self.cpc_best_path()) # add summary targets for this currency pair\n",
        "        self.calc_performances() # calculate performances based on targets\n",
        "        self.tf_vectors = TfVectors(tf=self, currency_pair=cur_pair)\n",
        "\n",
        "\n",
        "    def add_period_specific_targets(self, time_agg):\n",
        "#    def add_period_specific_targets_rolling(self, time_agg):\n",
        "        \"target = achieved if improvement > 1% without intermediate loss of more than 0.2%\"\n",
        "\n",
        "        print(f\"{datetime.now()}: add_period_specific_targets {time_agg}\")\n",
        "        df = self.tf_aggs[time_agg]\n",
        "        df['delta'] = 0.\n",
        "        df['target'] = \"-\"\n",
        "        pix = df.columns.get_loc('delta') # performance column index\n",
        "        lix = df.columns.get_loc('target')\n",
        "        cix = df.columns.get_loc('close')\n",
        "        win = dict()\n",
        "        loss = dict()\n",
        "        lossix = dict()\n",
        "        winix = dict()\n",
        "        lasttarget = dict()\n",
        "        for slot in range(0, time_agg):\n",
        "            win[slot] = loss[slot] = 0.\n",
        "            winix[slot] = lossix[slot] = slot\n",
        "            lasttarget[slot] = \"-\"\n",
        "#        for tix in range(((len(df)-1) % time_agg)+time_agg, len(df), time_agg): # tix = time index\n",
        "        for tix in range(time_agg, len(df), 1): # tix = time index\n",
        "            slot = (tix % time_agg)\n",
        "            last_close = df.iat[tix - time_agg, cix]\n",
        "            delta = (df.iat[tix, cix] - last_close) / last_close * 1000 #  in per mille: 1% == 10\n",
        "            df.iat[tix, pix] = delta\n",
        "            if delta < 0:\n",
        "                if loss[slot] < 0: # loss monitoring is running\n",
        "                    loss[slot] += delta\n",
        "                else: # first time bar of decrease period\n",
        "                    lossix[slot] = tix\n",
        "                    loss[slot] = delta\n",
        "                if win[slot] > 0: # win monitoring is running\n",
        "                    win[slot] += delta\n",
        "                    if win[slot] < 0: # reset win monitor because it is below start price\n",
        "                        win[slot] = 0.\n",
        "                if loss[slot] < SELL_THRESHOLD: # reset win monitor because dip exceeded threshold\n",
        "                    win[slot] = 0.\n",
        "#                    if lasttarget[slot] != \"sell\": # only one signal without further repeat\n",
        "                    df.iat[lossix[slot], lix] = lasttarget[slot] = \"sell\"\n",
        "                    lossix[slot] += 1 # allow multiple signals if conditions hold\n",
        "            elif delta > 0:\n",
        "                if win[slot] > 0: # win monitoring is running\n",
        "                    win[slot] += delta\n",
        "                else: # first time bar of increase period\n",
        "                    winix[slot] = tix\n",
        "                    win[slot] = delta\n",
        "                if loss[slot] < 0: # loss monitoring is running\n",
        "                    loss[slot] += delta\n",
        "                    if loss[slot] > 0:\n",
        "                        loss[slot] = 0. # reset loss monitor as it recovered before  sell threshold\n",
        "                if win[slot] > BUY_THRESHOLD: # reset win monitor because dip exceeded threshold\n",
        "                    loss[slot] = 0.\n",
        "#                    if lasttarget[slot] != \"buy\": # only one signal without further repeat\n",
        "                    df.iat[winix[slot], lix] = lasttarget[slot] = \"buy\"\n",
        "                    winix[slot] += 1 # allow multiple signals if conditions hold\n",
        "\n",
        "\n",
        "\n",
        "    def derive_features(self, time_agg):\n",
        "        \"derived features in relation to price based on the provided time aggregated dataframe df\"\n",
        "        # price deltas in 1/1000\n",
        "        df = self.tf_aggs[time_agg]\n",
        "        df['height'] = (df['high'] - df['low']) / df['close'] * 1000\n",
        "        df.loc[df['close'] > df['open'], 'top'] = (df['high'] - df['close']) / df['close'] * 1000\n",
        "        df.loc[df['close'] <= df['open'], 'top'] = (df['high'] - df['open']) / df['close'] * 1000\n",
        "        df.loc[df['close'] > df['open'], 'bottom'] = (df['open'] - df['low']) / df['close'] * 1000\n",
        "        df.loc[df['close'] <= df['open'], 'bottom'] = (df['close'] - df['low']) / df['close'] * 1000\n",
        "\n",
        "\n",
        "\n",
        "#    def time_aggregation_rolling(self):\n",
        "    def time_aggregation(self):\n",
        "        \"\"\"Time aggregation through rolling aggregation with the consequence that new data is\n",
        "        generated every minute and even long time aggregations reflect all minute bumps in their\n",
        "        features\n",
        "\n",
        "        in:\n",
        "            dataframe of minute data of a currency pair;\n",
        "        out:\n",
        "            dict of dataframes of aggregations with features and targets\n",
        "        \"\"\"\n",
        "        for time_agg in self.time_aggregations:\n",
        "            print(f\"{datetime.now()}: time_aggregation {time_agg}\")\n",
        "            if isinstance(time_agg, int):\n",
        "                if time_agg == 1:\n",
        "                    mdf = df = self.minute_data  # .copy()\n",
        "                    df['vol'] = (df['volume'] - df.volume.rolling(VOL_BASE_PERIOD).median()) \\\n",
        "                                 / df.volume.rolling(VOL_BASE_PERIOD).median()\n",
        "                else:\n",
        "                    df = pd.DataFrame()\n",
        "                    df['close'] = mdf.close\n",
        "                    df['high'] = mdf.high.rolling(time_agg).max()\n",
        "                    df['low'] = mdf.low.rolling(time_agg).min()\n",
        "                    df['open'] = mdf.open.shift(time_agg-1)\n",
        "                    df['vol'] = mdf.vol.rolling(time_agg).mean()\n",
        "                self.tf_aggs[time_agg] = df\n",
        "                self.derive_features(time_agg)\n",
        "\n",
        "\n",
        "\n",
        "    def cpc_best_path(self):\n",
        "        \"\"\"identify best path through all aggregations - use all buy and sell signals.\n",
        "        The search space is limited because costs increase the same for all paths.\n",
        "        Hence, one has only to check the most recent 2 buy signals and\n",
        "        the most recent 2 sell signals for maximization.\n",
        "\n",
        "        1. no holding with buy signal:\n",
        "            open potential buy\n",
        "        2. holding, old open potential sell with sell signal:\n",
        "            current performance - fee >= potential transaction performance:\n",
        "                discard old potential sell and open new potential sell\n",
        "            else:\n",
        "                ignore new sell signal\n",
        "                if potential transaction performance > 0:\n",
        "                    execute potential sell and remove corresponding buy and sell\n",
        "        3. holding, no open potential sell with sell signal:\n",
        "            open potential sell\n",
        "        4. holding, with buy signal:\n",
        "            current performance < -fee:\n",
        "                if potential transaction performance > 0:\n",
        "                    execute potential sell and remove corresponding buy and sell\n",
        "                discard old potential buy and open new potential buy\n",
        "                discard any potential sell including new ones\n",
        "                current performance = -fee\n",
        "            current performance >= -fee: do nothing, i.e.\n",
        "                discard new buy and hold on to old potential buy\n",
        "                hold on to any potential sells including new ones\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        cpc_df = pd.DataFrame(self.minute_data, columns=['close'])\n",
        "        cpc_df[CPC+'_target'] = HOLD\n",
        "        target_ix = cpc_df.columns.get_loc(CPC+'_target')\n",
        "        close_ix = cpc_df.columns.get_loc('close')\n",
        "        col_ix = dict()\n",
        "        for time_agg in self.time_aggregations:\n",
        "            if isinstance(time_agg, int):\n",
        "                c_name = str(time_agg) + '_target'\n",
        "                cpc_df[c_name] = self.tf_aggs[time_agg].target\n",
        "                col_ix[time_agg] = cpc_df.columns.get_loc(c_name)\n",
        "        assert cpc_df.index.is_unique, \"unexpected not unique index\"\n",
        "        print(f\"{datetime.now()}: best path\")\n",
        "        holding = False\n",
        "        pot_transaction = False # becomes true if potential buy and potential sell are detected\n",
        "        transaction_perf = current_perf = best_perf = 0.\n",
        "        buy_tix = sell_tix = 0\n",
        "        last = cpc_df.iat[0, close_ix]\n",
        "        for tix in range(len(cpc_df)): # tix = time index\n",
        "            this = cpc_df.iat[tix, close_ix]\n",
        "            tix_perf = ((this - last)/ last * 1000)\n",
        "            last = this\n",
        "            sell_sig = False\n",
        "            buy_sig = False\n",
        "            for six in col_ix:\n",
        "                signal = cpc_df.iat[tix, col_ix[six]]\n",
        "                if signal == BUY:\n",
        "                    buy_sig = True\n",
        "                if signal == SELL:\n",
        "                    sell_sig = True\n",
        "            if holding:\n",
        "                current_perf += tix_perf\n",
        "                if sell_sig:\n",
        "                    if pot_transaction: # use case 2\n",
        "                        if (current_perf - FEE) >= transaction_perf: # reset transaction\n",
        "                            transaction_perf = current_perf - FEE\n",
        "                            sell_tix = tix\n",
        "                            pot_transaction = True # remains True\n",
        "                        else:\n",
        "                            if transaction_perf > 0: # execute transaction\n",
        "#                                assert buy_tix < sell_tix, \\\n",
        "#                                    f\"inconsistent tix marks, buy: {buy_tix} sell: {sell_tix}\"\n",
        "                                best_perf += transaction_perf\n",
        "                                cpc_df.iat[buy_tix, target_ix] = BUY\n",
        "                                cpc_df.iat[sell_tix, target_ix] = SELL\n",
        "                            pot_transaction = False\n",
        "                            holding = False\n",
        "                            current_perf = transaction_perf = 0.\n",
        "                    else: # use case 3 = no potential transaction\n",
        "                        transaction_perf = current_perf - FEE\n",
        "                        sell_tix = tix\n",
        "                        pot_transaction = True # remains True\n",
        "#                        assert buy_tix < sell_tix, \"inconsistent buy/sell tix marks\"\n",
        "                if buy_sig: # use case 4, from different time aggs there can be sell and buy!\n",
        "                    if current_perf < -FEE: # set new buy_tix but check for a potential tr.\n",
        "                        if pot_transaction and (transaction_perf > 0): # execute transaction\n",
        "#                            assert buy_tix < sell_tix, \\\n",
        "#                                f\"inconsistent tix marks, buy: {buy_tix} sell: {sell_tix}\"\n",
        "                            best_perf += transaction_perf\n",
        "                            cpc_df.iat[buy_tix, target_ix] = BUY\n",
        "                            cpc_df.iat[sell_tix, target_ix] = SELL\n",
        "                            pot_transaction = False\n",
        "                            holding = False\n",
        "                            transaction_perf = 0.\n",
        "                        buy_tix = tix\n",
        "                        current_perf = -FEE\n",
        "                        holding = True\n",
        "            else: # not holding\n",
        "                if buy_sig: # use case 1 = not holding with buy signal\n",
        "                    buy_tix = tix\n",
        "                    current_perf = -FEE\n",
        "                    holding = True\n",
        "                    pot_transaction = False\n",
        "        self.tf_aggs[CPC] = cpc_df\n",
        "        return best_perf\n",
        "\n",
        "\n",
        "\n",
        "    def calc_performances(self):\n",
        "        \"\"\"calculate all time aggregation specific performances\n",
        "        as well as the CPC summary best path performance.\n",
        "        \"\"\"\n",
        "\n",
        "        perf = dict()\n",
        "        print(f\"{datetime.now()}: calculate performances\")\n",
        "        cpc_df = self.tf_aggs[CPC]\n",
        "        col_ix = dict()\n",
        "        ta_holding = dict()\n",
        "        for time_agg in self.time_aggregations:\n",
        "            perf[time_agg] = 0.\n",
        "            ta_holding[time_agg] = False\n",
        "#            self.tf_aggs[time_agg].loc[:, 'perf'] = 0.\n",
        "            t_name = str(time_agg) + '_target'\n",
        "            col_ix[time_agg] = cpc_df.columns.get_loc(t_name)\n",
        "            assert col_ix[time_agg] > 0, f\"did not find column {col_ix[time_agg]} of {time_agg}\"\n",
        "        close_ix = cpc_df.columns.get_loc('close')\n",
        "\n",
        "        assert cpc_df.index.is_unique, \"unexpected not unique index\"\n",
        "        last = cpc_df.iat[0, close_ix]\n",
        "        for tix in range(len(cpc_df)): # tix = time index\n",
        "            this = cpc_df.iat[tix, close_ix]\n",
        "            tix_perf = ((this - last)/ last * 1000)\n",
        "            last = this\n",
        "            for ta_ix in perf:\n",
        "                signal = cpc_df.iat[tix, col_ix[ta_ix]]\n",
        "                if ta_holding[ta_ix]:\n",
        "                    perf[ta_ix] += tix_perf\n",
        "#                    self.tf_aggs[time_agg].at[tic, 'perf'] = perf[time_agg]\n",
        "                if (signal == BUY) and (not ta_holding[ta_ix]):\n",
        "                    perf[ta_ix] -= FEE\n",
        "#                    self.tf_aggs[time_agg].at[tic, 'perf'] = perf[time_agg]\n",
        "                    ta_holding[ta_ix] = True\n",
        "                if (signal == SELL) and ta_holding[ta_ix]:\n",
        "                    perf[ta_ix] -= FEE\n",
        "#                    self.tf_aggs[time_agg].at[tic, 'perf'] = perf[time_agg]\n",
        "                    ta_holding[ta_ix] = False\n",
        "        self.performance = perf\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "class TfVectors:\n",
        "    \"\"\"Container class for targets and features of a currency pair\n",
        "\n",
        "    attributes:\n",
        "        currency pair - pair used to derive targets and features\n",
        "        vecs - dict with either CPC as summary label or time aggregation in  minutes as keys\n",
        "        and corresponding data frames as dict items.\n",
        "\n",
        "\n",
        "    feature vector dataframe columns:\n",
        "        timeseries index in minutes of data used\n",
        "\n",
        "        buy - 1 if buy signal, 0 otherwise\n",
        "\n",
        "        sell - 1 if sell signal, 0 otherwise\n",
        "\n",
        "        series of delta (D), height (H), top(T), bottom (B), volume (V)\n",
        "        each column name encodes the aggregation in minutes first, followed by the series nbr,\n",
        "        followed by a single character DHTBV indicating the data element\n",
        "    \"\"\"\n",
        "\n",
        "#    def build_classifier_vectors(self):\n",
        "    def __init__(self, tf=None, filename=None, currency_pair=None):\n",
        "        \"\"\"Builds a target and feature vector sequence with DHTBV feature sequences of\n",
        "        n time steps (tics) as configured in time_aggregations in T units\n",
        "\n",
        "        Result:\n",
        "            a self.vecs dict with keys as in time_aggregations plus 'CPC' that\n",
        "            are referring to DataFrames with feature vectors as rows. The column name indicates\n",
        "            the type of feature, i.e. either 'target' or 'aggregation_tic_D|H|T|B|V'\n",
        "\n",
        "            There will be a different number of feature vectors per aggregation due to the\n",
        "            nan in the beginning rows that have missing history vectors before them.\n",
        "        \"\"\"\n",
        "        self.cur_pair = currency_pair\n",
        "        self.vecs = dict() # full fleged feature vectors and their targets\n",
        "        self.data_version = 1.0\n",
        "        self.filename = filename\n",
        "        if tf is not None:\n",
        "            if currency_pair is not None:\n",
        "                print(f\"{datetime.now()}: processing {self.cur_pair}\")\n",
        "            self.aggregations = tf.time_aggregations\n",
        "            for ta in tf.time_aggregations:\n",
        "                print(f\"{datetime.now()}: build classifier vectors {ta}\")\n",
        "                t_name = str(ta) + '_target'\n",
        "                self.vecs[ta] = pd.DataFrame(tf.minute_data, columns=['close', 'buy', 'sell'])\n",
        "                self.vecs[ta]['buy'] = (tf.tf_aggs[CPC][t_name] == 'buy').astype(float)\n",
        "                self.vecs[ta]['sell'] = (tf.tf_aggs[CPC][t_name] == 'sell').astype(float)\n",
        "                for tics in range(tf.time_aggregations[ta]):\n",
        "                    tgt = str(ta) + 'T_' + str(tics) + '_'\n",
        "                    if isinstance(ta, int):\n",
        "                        offset = tics*ta\n",
        "                    else:\n",
        "                        offset = tics\n",
        "                    self.vecs[ta][tgt + 'D'] = tf.tf_aggs[ta].delta.shift(offset)\n",
        "                    self.vecs[ta][tgt + 'H'] = tf.tf_aggs[ta].height.shift(offset)\n",
        "                    self.vecs[ta][tgt + 'T'] = tf.tf_aggs[ta].top.shift(offset)\n",
        "                    self.vecs[ta][tgt + 'B'] = tf.tf_aggs[ta].bottom.shift(offset)\n",
        "                    self.vecs[ta][tgt + 'V'] = tf.tf_aggs[ta].vol.shift(offset)\n",
        "                self.vecs[ta].dropna(inplace=True)\n",
        "                assert not self.vecs[ta].empty, \"empty dataframe from TargetsFeatures\"\n",
        "        elif filename is not None:\n",
        "            self.filename = None\n",
        "            df_f = open(filename, 'rb')\n",
        "            self.data_version, self.cur_pair, self.vecs, self.filename, self.aggregations \\\n",
        "                = pickle.load(df_f)\n",
        "            print(f\"{datetime.now()}: processing {self.cur_pair}\")\n",
        "            print(\"{}: read tf vectors with {} tics x {} aggregations from {}\".format( \\\n",
        "                     datetime.now(), len(self.vecs[CPC]), len(self.vecs), filename))\n",
        "            df_f.close()\n",
        "\n",
        "    def vec(self, key):\n",
        "        \"Returns the dataframe of the given key\"\n",
        "        return self.vecs[key]\n",
        "\n",
        "    def set_pair_name(self, currency_pair):\n",
        "        \"sets the currency pair name\"\n",
        "        self.cur_pair = currency_pair\n",
        "\n",
        "    def pair_name(self):\n",
        "        \"returns the currency pair name\"\n",
        "        return self.cur_pair\n",
        "\n",
        "    def save(self, fname):\n",
        "        \"saves the object via pickle\"\n",
        "        print(\"{}: writing tf vectors with {} tics ({} - {}) x {} aggregations to {}\".format( \\\n",
        "                 datetime.now(), len(self.vecs[CPC]), self.vecs[CPC].index[0],\\\n",
        "                 self.vecs[CPC].index[len(self.vecs[CPC])-1], len(self.vecs), fname))\n",
        "        self.filename = fname\n",
        "        df_f = open(fname, 'wb')\n",
        "        pickle.dump((self.data_version, self.cur_pair, self.vecs, self.filename, \\\n",
        "                     self.aggregations), df_f)\n",
        "        df_f.close()\n",
        "        print(f\"{datetime.now()}: tf vectors saved\")\n",
        "\n",
        "    def signal_sequences(self, key):\n",
        "        \"provides a histogram of consecutive signals for the given key data\"\n",
        "        c_df = self.vecs[key]\n",
        "        b_seq = dict()\n",
        "        s_seq = dict()\n",
        "        h_seq = dict()\n",
        "        seq = {'buy': b_seq, 'sell': s_seq, 'hold': h_seq}\n",
        "        b_count = s_count = h_count = 0\n",
        "        b_col = c_df.columns.get_loc('buy')\n",
        "        s_col = c_df.columns.get_loc('sell')\n",
        "        for t_ix in range(len(c_df)):\n",
        "            b_sig = c_df.iat[t_ix, b_col]\n",
        "            if b_sig > 0:\n",
        "                b_count += 1\n",
        "                assert b_sig == 1, f\"unexpected buy signal value: {b_sig}\"\n",
        "            else:\n",
        "                if b_count > 0:\n",
        "                    if b_count in b_seq:\n",
        "                        b_seq[b_count] += 1\n",
        "                    else:\n",
        "                        b_seq[b_count] = 1\n",
        "                    b_count = 0\n",
        "            s_sig = c_df.iat[t_ix, s_col]\n",
        "            if s_sig > 0:\n",
        "                s_count += 1\n",
        "                assert s_sig == 1, f\"unexpected sell signal value: {s_sig}\"\n",
        "            else:\n",
        "                if s_count > 0:\n",
        "                    if s_count in s_seq:\n",
        "                        s_seq[s_count] += 1\n",
        "                    else:\n",
        "                        s_seq[s_count] = 1\n",
        "                    s_count = 0\n",
        "            assert not ((b_sig > 0) and (s_sig > 0)), f\"unexpected double signal sell and buy\"\n",
        "            if (b_sig == 0) and (s_sig == 0):\n",
        "                h_count += 1\n",
        "            else:\n",
        "                if h_count > 0:\n",
        "                    if h_count in h_seq:\n",
        "                        h_seq[h_count] += 1\n",
        "                    else:\n",
        "                        h_seq[h_count] = 1\n",
        "                    h_count = 0\n",
        "        return seq\n",
        "\n",
        "\n",
        "    def split_data(self, key, train_ratio=0.6, val_ratio=0.2, hold_ratio=1):\n",
        "        \"\"\"Splits the data set into training set, validation set and test set such that\n",
        "        training set receives at least the specified ratio of buy and sell samples and\n",
        "        validation set also receives at least the specified ratio of buy and sell samples.\n",
        "        sequences of buy or sell samples will be reduced every n of the sequence according\n",
        "        to the aggregation value, e.g. if 1:10 is specified then every 10th buy or sell sample\n",
        "        is used from a consecutive sequence.\n",
        "        hold_ratio is the ratio of hold / (sell + buy)\n",
        "\n",
        "        Returns:\n",
        "        ========\n",
        "        dict with dataframes for a training, a validation and a test set\n",
        "\n",
        "        Attention for possible label leakage:\n",
        "        =====================================\n",
        "        buy, sell and hold samples can overlap in time range\n",
        "        \"\"\"\n",
        "        c_df = self.vecs[key]\n",
        "        b_count = s_count = h_count = 0\n",
        "        bt_count = st_count = 0\n",
        "        bv_count = sv_count = seq_count = 0\n",
        "#        agg = max(self.aggregations[key], 1) # use only every agg sample in signal sequences\n",
        "        agg = 4 # use only every agg sample in signal sequences\n",
        "        b_col = c_df.columns.get_loc('buy')\n",
        "        s_col = c_df.columns.get_loc('sell')\n",
        "        c_df['select'] = '-'\n",
        "        sel_col = c_df.columns.get_loc('select')\n",
        "        last = 'x'\n",
        "        for t_ix in range(len(c_df)):\n",
        "            b_sig = c_df.iat[t_ix, b_col]\n",
        "            if b_sig > 0:\n",
        "                if last == 'buy':\n",
        "                    seq_count += 1\n",
        "                else:\n",
        "                    seq_count = 0\n",
        "                    last = 'buy'\n",
        "                if (seq_count % agg) == 0:\n",
        "                    c_df.iat[t_ix, sel_col] = 'buy_select'\n",
        "                    b_count += 1\n",
        "\n",
        "            s_sig = c_df.iat[t_ix, s_col]\n",
        "            if s_sig > 0:\n",
        "                if last == 'sell':\n",
        "                    seq_count += 1\n",
        "                else:\n",
        "                    seq_count = 0\n",
        "                    last = 'sell'\n",
        "                if (seq_count % agg) == 0:\n",
        "                    c_df.iat[t_ix, sel_col] = 'sell_select'\n",
        "                    s_count += 1\n",
        "\n",
        "            if (b_sig == 0) and (s_sig == 0):\n",
        "                if last == 'hold':\n",
        "                    seq_count += 1\n",
        "                else:\n",
        "                    seq_count = 0\n",
        "                    last = 'hold'\n",
        "                if (seq_count % agg) == 0:\n",
        "                    c_df.iat[t_ix, sel_col] = 'hold_select'\n",
        "                    h_count += 1\n",
        "\n",
        "        h_ratio = math.floor((h_count / (b_count + s_count)) / hold_ratio)\n",
        "        h_count = 0\n",
        "        for t_ix in range(len(c_df)):\n",
        "            this = c_df.iat[t_ix, sel_col]\n",
        "            if this == 'buy_select':\n",
        "                if (bt_count / b_count) < train_ratio:\n",
        "                    c_df.iat[t_ix, sel_col] = 'train'\n",
        "                    bt_count += 1\n",
        "                elif (bv_count / b_count) < val_ratio:\n",
        "                    c_df.iat[t_ix, sel_col] = 'val'\n",
        "                    bv_count += 1\n",
        "                else:\n",
        "                    c_df.iat[t_ix, sel_col] = 'test'\n",
        "            elif this == 'sell_select':\n",
        "                if (st_count / s_count) < train_ratio:\n",
        "                    c_df.iat[t_ix, sel_col] = 'train'\n",
        "                    st_count += 1\n",
        "                elif (sv_count / s_count) < val_ratio:\n",
        "                    c_df.iat[t_ix, sel_col] = 'val'\n",
        "                    sv_count += 1\n",
        "                else:\n",
        "                    c_df.iat[t_ix, sel_col] = 'test'\n",
        "            elif this == 'hold_select':\n",
        "                if (h_count % h_ratio) == 0:\n",
        "                    if ((bt_count + st_count) / (b_count + s_count)) < train_ratio:\n",
        "                        c_df.iat[t_ix, sel_col] = 'train'\n",
        "                    elif ((bv_count + sv_count) / (b_count + s_count)) < val_ratio:\n",
        "                        c_df.iat[t_ix, sel_col] = 'val'\n",
        "                    else:\n",
        "                        c_df.iat[t_ix, sel_col] = 'test'\n",
        "                h_count += 1\n",
        "\n",
        "        train_df = c_df.loc[c_df['select'] == 'train']\n",
        "        val_df = c_df.loc[c_df['select'] == 'val']\n",
        "        test_df = c_df.loc[c_df['select'] == 'test']\n",
        "        c_df.drop(['select'], axis = 1, inplace = True)\n",
        "        train_df.drop(['select'], axis = 1, inplace = True)\n",
        "        val_df.drop(['select'], axis = 1, inplace = True)\n",
        "        test_df.drop(['select'], axis = 1, inplace = True)\n",
        "        seq = {'training': train_df, 'validation': val_df, 'test': test_df}\n",
        "        return seq\n",
        "\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    import sys\n",
        "#    fib(int(sys.argv[1]))\n",
        "#    currency_data = FeaturesTargets()\n",
        "#    print(currency_data.performances())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}